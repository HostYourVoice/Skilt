"""ChatGPT-Powered AI Feedback System"""

from pydantic import BaseModel, create_model
from openai import OpenAI
import rich
import sys
import random

from utils import get_openai_api_key, get_logger

log = get_logger(log_level_override="INFO")

MODEL_PROMPT_TEMPLATE = """You are an AI feedback system. Your task is to provide feedback on a piece of writing based on a rubric.
You will be given a piece of text, a list of rubric points, and a description of what needs to be written about (evaluation themes).
Your response should include a rating for each rubric point, as well as a response to the evaluation themes in the form of a summary 
of the text and a description of what needs improvement. The ratings should be numbers between 1 and 5, where 1 is the lowest and 5 is the highest.
Make sure your feedback aligns with the provided evaluation themes:\n{evaluation_themes}"""

OPENAI_MODEL = "gpt-4o-mini"


def generate_schema(rubric_points: list[str]) -> dict:
    """
    Generate a schema for the feedback model.

    Args:
        rubric_points: List of rubric points to be rated numerically.

    Returns:
        dict: Schema for the feedback model.
    """
    return create_model(
        "FeedbackModel",
        general_evaluation=(str, ...),
        **{rubric_point: (float, ...) for rubric_point in rubric_points},
    )


def generate_feedback(
    text: str, feedback_schema: type[BaseModel], evaluation_themes: str
):
    """
    Generate feedback for a text using the OpenAI API.

    Args:
        text: The text to generate feedback for
        feedback_schema: The Pydantic model to use for parsing the response
        evaluation_themes: Description of what needs to be written about.

    Returns:
        dict: The parsed feedback
    """
    client = OpenAI(api_key=get_openai_api_key())

    log.info(f"Generating feedback for text: {text}")
    completion = client.beta.chat.completions.parse(
        model="gpt-4o-2024-08-06",
        messages=[
            {
                "role": "system",
                "content": MODEL_PROMPT_TEMPLATE.format(
                    evaluation_themes=evaluation_themes
                ),
            },
            {"role": "user", "content": f"Please evaluate this text:\n\n{text}"},
        ],
        response_format=feedback_schema,
    )

    feedback = completion.choices[0].message.parsed.model_dump()
    log.info(f"Generated feedback: {feedback}")
    return feedback


def ai_feedback(
    text: str,
    evaluation_themes: str,
    rubric_points: list[str],
    mock: bool = True,  # Leaving as True for now for testing purposes
) -> dict:
    """
    Generate AI feedback.

    Args:
        text: Text to be evaluated.
            Example: "The whales are an important part of the ecosystem. We must protect them."

        evaluation_themes: Description of what needs to be written about.
            Example: "A piece of writing that is well-structured and
            uses appropriate evidence to support why society must protect the whales."

        rubric_points: List of rubric points to be rated numerically.
            Example: ["Word choice was done well",
            "Used appropriate evidence",
            "Used active voice"]

        mock: Whether to use mock data for testing purposes. Default is True.

    Returns:
        dict: Feedback generated by the AI. Example:
        {
            "general_evaluation": "This is a well-written piece of writing.",
            "Word choice was done well": 5,
            "Used appropriate evidence": 4,
            "Used active voice": 3,
        }
    """
    log.debug(f"Received text: {text}")
    log.debug(f"Received evaluation themes: {evaluation_themes}")
    log.debug(f"Received rubric points: {rubric_points}")
    log.debug(f"Received mock flag: {mock}")

    # Generate the feedback schema
    feedback_schema = generate_schema(rubric_points)
    log.debug(f"Generated feedback schema: {feedback_schema.model_fields}")

    # Generate feedback using the OpenAI API
    if mock:
        feedback = {
            "general_evaluation": "This is a well-written piece of writing!",
            **{rubric_point: random.randint(1, 5) for rubric_point in rubric_points},
        }
    else:
        feedback = generate_feedback(text, feedback_schema, evaluation_themes)

    return feedback


if __name__ == "__main__":
    import ast

    feedback = ai_feedback(sys.argv[1], sys.argv[2], ast.literal_eval(sys.argv[3]))
    rich.print(feedback)
